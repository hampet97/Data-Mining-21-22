{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff0ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: library imports are ALWAYS at the top of the script, no exceptions!\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6717443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to database\n",
    "my_path = os.path.join(\"..\", \"data\", \"datamining.db\")\n",
    "\n",
    "# connect to the database\n",
    "conn = sqlite3.connect(my_path)\n",
    "\n",
    "# the query\n",
    "query = \"\"\"\n",
    "select\n",
    "    age, \n",
    "    income, \n",
    "    frq, \n",
    "    rcn, \n",
    "    mnt, \n",
    "    clothes, \n",
    "    kitchen, \n",
    "    small_appliances, \n",
    "    toys, \n",
    "    house_keeping,\n",
    "    dependents, \n",
    "    per_net_purchase,\n",
    "    g.gender, \n",
    "    e.education, \n",
    "    m.status, \n",
    "    r.description\n",
    "from customers as c\n",
    "    join genders as g on g.id = c.gender_id\n",
    "    join education_levels as e on e.id = c.education_id\n",
    "    join marital_status as m on m.id = c.marital_status_id\n",
    "    join recommendations as r on r.id = c.recommendation_id\n",
    "order by c.id;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a591fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features = [\"education\", \"status\", \"gender\", \"dependents\", \"description\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750678b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c31c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataset\n",
    "df_central = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bfd767",
   "metadata": {},
   "source": [
    "- if you have more then 20% missing vals in a variable -> remove it (rule of thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa72c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "income              0\n",
       "frq                 0\n",
       "rcn                 0\n",
       "mnt                 0\n",
       "clothes             0\n",
       "kitchen             0\n",
       "small_appliances    0\n",
       "toys                0\n",
       "house_keeping       0\n",
       "dependents          0\n",
       "per_net_purchase    0\n",
       "gender              0\n",
       "education           0\n",
       "status              0\n",
       "description         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_central.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3e9c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hampe\\Anaconda3\\envs\\datamining\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                 1966.0\n",
       "frq                   17.0\n",
       "rcn                   53.0\n",
       "mnt                  383.0\n",
       "clothes               51.0\n",
       "kitchen                4.0\n",
       "small_appliances      28.0\n",
       "toys                   4.0\n",
       "house_keeping          4.0\n",
       "per_net_purchase      45.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_central.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17384db6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_13572/2259413871.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\hampe\\AppData\\Local\\Temp/ipykernel_13572/2259413871.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    -> mode\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# what central tendency method to use in case of missing vals in categorcial var? \n",
    "    -> mode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a786f9",
   "metadata": {},
   "source": [
    "## Fill missing values\n",
    "\n",
    "- fill missing vals using median for numerical vars and using the mode for categroical and ordinal vars\n",
    "    - concat the medians and means for each var -> df.fillna()\n",
    "- other approach to fill missing vals: \n",
    "    - knn imputation -> can be used only for numerical vars (distance makes no sense with cat and ordinal vars)\n",
    "- knn is a \"lazy learner\": no steps are needed to do the predicitons (it jsut needs the dataset) (e.g: naive bayes is not like this) \n",
    "- should we standadize data before applying knn? \n",
    "    -> yes!!!!\n",
    "    - if A has vals around 1000 and B around 1: if i look for closest neighbors i basically just look for similar A vals...\n",
    "- how to select the number of neighbors to use in knn method?\n",
    "    - grid search -> evaluate how good a specific parameter combination is\n",
    "- considering outliers when applying the knn???\n",
    "    -e.g.: if var A has an outlier that has missing vals: if i look for its nearest neighbors, i am actually considering other outliers to fill in the missing vals of A\n",
    "    -> remove outliers before applying knn!!\n",
    "    \n",
    "- only numerical var with missing values is income \n",
    "    - fill it with median / fill it using knn\n",
    "        - sometimes it makes sense to simply use the median, but knowing what income means and having so much information about user profiles, it makes more sense to use knn\n",
    "\n",
    "- should we use knn on binary (0/1) features -> No! (distance here can be calculated, but again doesnt make sense) \n",
    "\n",
    "\n",
    "\n",
    "## Remove Outliers\n",
    "- plot histogram for each var -> which one do we think might has outliers? \n",
    "    - create a filter to only keep inliers (set the treshold manually for each variable (that we think does have outliers) based on its histogram)\n",
    "    - kitchen, toys and housekeeping have very similar profiles \n",
    "        - if i consider outliers in one of them, i should be following the same approach with the others\n",
    "- ideally how much of the dataset can we remove? what would be too much to remove? \n",
    "    - we may remove less then 3 percent (rule of thumb)\n",
    "    - more than 5 is way too much\n",
    "    - 2 would be great\n",
    "    - 0 would be ideal (means we have no outliers) \n",
    "- rcn var: very long tail\n",
    "    - removing the whole tail means removing too many outliers \n",
    "        - we could set a treshold: everthing above 100: we just set it to 100\n",
    "        - also, looking at the scatterplots (pairwise correlatinos) we can see  that rcn has no correlations with others, it just seems to be random -> we'll probably not use this feature anyway later in the analysis \n",
    "            - but isnt the whole point to use uncorrelated vars? \n",
    "                - you wanna avoid completely correlated vars\n",
    "                - you also dont wanna use completely uncorrelated vars\n",
    "                    - vars describing the same phenomena from different aspects -> we expect them to have some correlation (but we dont want them two be 95% correlated, that would just mean they hold the exact same information)\n",
    "- remove outliers with IQR method (inter quantile range) \n",
    "    - df.quantile(.25) -> Q1 \n",
    "    - compute IQR value for each feature \n",
    "    - calculate lower and upper limit for each var \n",
    "    \n",
    "    - apply it -> we see that it would remove 18% of the dataset \n",
    "        -> this is way too much -> adjus the IQR method: instead of 1.5, multiply with a larger value which would result in less outliers\n",
    "\n",
    "\n",
    "- note: if we wanted to use knn for filling nans: the outlier neglection should be the first step!!\n",
    "        \n",
    "## Feature extraction\n",
    "\n",
    "- homework: explore -> try to extract meaningful feature\n",
    "    - manipulations of the current features that make more sense or can be useful (e.g: age instead of year of birth, avg amount spent per visit) \n",
    "    \n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
